"""ç ”ç©¶æ­¥éª¤"""

import re
import json
import time
import asyncio  # æ–°å¢ï¼šç”¨äºåŒæ­¥è°ƒç”¨å¼‚æ­¥æ–¹æ³•
from pathlib import Path
from typing import Any, Dict, List, Optional
from datetime import datetime
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from ymda.pipeline.steps.validate_step import BaseStep
from ymda.llm.deep_research_agent import Deep_ResearchAgent
from ymda.data.models import ResearchRun  # æ–°å¢
from ymda.data.repository import get_repository  # æ–°å¢
from ymda.settings import Settings
from ymda.utils.logger import get_logger
from ymda.utils.schema_utils import flatten_expected_fields

logger = get_logger(__name__)


class ResearchStep(BaseStep):
    """ç ”ç©¶æ­¥éª¤ - ä½¿ç”¨ Deep_Research (LangGraph) è¿›è¡Œæ·±åº¦ç ”ç©¶"""
    
    LANGUAGE_REQUIREMENT_TAG = "[LANGUAGE REQUIREMENT]"
    LANGUAGE_REQUIREMENT_BLOCK = (
        "[LANGUAGE REQUIREMENT]\n"
        "You MUST deliver the entire research report, structured outputs, provenance evidence_text, and any explanations strictly in English. "
        "If your sources are not in English, summarize and translate them into English while preserving product names, numbers, and units."
    )
    ENGLISH_RETRY_BLOCK = (
        "[ENGLISH_ONLY_RETRY]\n"
        "The previous answer was not fully in English. Rewrite the whole report, structured data, and provenance strictly in English. "
        "Translate all content into English while keeping key terms intact."
    )
    NON_ENGLISH_PATTERN = re.compile(r'[\u4e00-\u9fff\u3040-\u30ff\uac00-\ud7af]')
    RATE_LIMIT_HINT_PATTERN = re.compile(r'try again in ([0-9.]+)s', re.IGNORECASE)
    RATE_LIMIT_MAX_RETRIES = 3
    RATE_LIMIT_INITIAL_DELAY = 8.0
    
    def __init__(self, settings: Settings):
        super().__init__(settings)
        # ä½¿ç”¨ OpenAI API for Deep_Research
        api_key = settings.openai_api_key
        
        self.deep_research_client = Deep_ResearchAgent(
            api_key=api_key,
            model="gpt-4.1-mini"
        )
        self._load_research_prompt()
        self.post_structure_llm = ChatOpenAI(
            model="gpt-4.1-mini",
            temperature=0,
            api_key=api_key
        )
    
    def _load_research_prompt(self):
        """åŠ è½½ç ”ç©¶Promptæ¨¡æ¿"""
        try:
            # è¿™é‡Œçš„è·¯å¾„å‡è®¾æ˜¯ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæˆ–è€…æŒ‰ç…§ç›¸å¯¹è·¯å¾„æ‰¾åˆ°
            prompt_template_path = Path(__file__).parent.parent.parent / "llm" / "prompts" / "research.md"
            
            if not prompt_template_path.exists():
                logger.warning(f"ç ”ç©¶Promptæ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {prompt_template_path}ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿")
                template_content = """è¯·å¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œç ”ç©¶ã€‚

## Yield Machine ä¿¡æ¯
- åç§°: {ym_name}
- ç±»åˆ«: {ym_category}
- æ‘˜è¦: {ym_summary}
- ä½¿ç”¨åœºæ™¯: {use_cases}

## é—®é¢˜ä¿¡æ¯
- é—®é¢˜: {question_text}
- ç›®æ ‡å­—æ®µ: {target_field}

## è¦æ±‚
è¯·åŸºäºæƒå¨æ¥æºï¼ˆç§‘æŠ€åª’ä½“ã€è¡Œä¸šæŠ¥å‘Šã€å®˜æ–¹æ–‡æ¡£ç­‰ï¼‰å›ç­”è¯¥é—®é¢˜ã€‚"""
            else:
                with open(prompt_template_path, 'r', encoding='utf-8') as f:
                    template_content = f.read()
            
            # æ„å»ºPromptæ¨¡æ¿
            self.research_prompt = PromptTemplate(
                template=template_content,
                input_variables=["ym_name", "ym_summary", "ym_category", "use_cases", 
                               "question_text", "question_type", "target_field"]
            )
        except Exception as e:
            logger.error(f"åŠ è½½ç ”ç©¶Promptæ¨¡æ¿å¤±è´¥: {e}")
            raise
    
    def _ensure_language_requirement(self, query: str) -> str:
        """åœ¨æŸ¥è¯¢æœ«å°¾é™„åŠ è‹±æ–‡è¾“å‡ºè¦æ±‚"""
        safe_query = (query or "").strip()
        if not safe_query:
            safe_query = "Research the specified Yield Machine question."
        
        if self.LANGUAGE_REQUIREMENT_TAG.lower() in safe_query.lower():
            return safe_query
        
        return f"{safe_query}\n\n{self.LANGUAGE_REQUIREMENT_BLOCK}"
    
    def _append_retry_instruction(self, query: str) -> str:
        """ä¸ºé‡è¯•è¯·æ±‚é™„åŠ æ›´å¼ºçš„è‹±æ–‡çº¦æŸ"""
        base_query = (query or "").rstrip()
        return f"{base_query}\n\n{self.ENGLISH_RETRY_BLOCK}"
    
    def _update_input_payload(self, run_id: int, repository, query: str):
        """å°†æœ€ç»ˆä½¿ç”¨çš„æŸ¥è¯¢å†™å› research_run"""
        try:
            repository.client.table('research_run')\
                .update({'input_payload': {'query': query}})\
                .eq('id', run_id)\
                .execute()
        except Exception as e:
            logger.warning(f"æ›´æ–° run {run_id} input_payload å¤±è´¥: {e}")
    
    def _execute_research_once(self, query: str, schema_wrapper: Dict[str, Any]) -> Dict[str, Any]:
        """åŒæ­¥æ‰§è¡Œä¸€æ¬¡ Deep Research"""
        return asyncio.run(
            self.deep_research_client.research(
                query=query,
                json_schema=schema_wrapper
            )
        )
    
    def _error_text(self, error: Exception) -> str:
        """æå–é”™è¯¯æ–‡æœ¬"""
        parts = []
        for attr in ('message', 'body'):
            value = getattr(error, attr, None)
            if value:
                parts.append(str(value))
        if hasattr(error, 'args') and error.args:
            parts.extend(str(arg) for arg in error.args)
        if not parts:
            parts.append(str(error))
        return " | ".join(part for part in parts if part)
    
    def _parse_retry_after(self, text: str) -> Optional[float]:
        match = self.RATE_LIMIT_HINT_PATTERN.search(text)
        if match:
            try:
                return max(float(match.group(1)), 0)
            except ValueError:
                return None
        return None
    
    def _should_retry_rate_limit(self, error: Exception) -> tuple[bool, Optional[float]]:
        text = self._error_text(error)
        lowered = text.lower()
        code = getattr(error, 'code', '')
        if isinstance(code, str):
            code = code.lower()
        if 'rate limit' in lowered or 'rate_limit' in lowered or code == 'rate_limit_exceeded':
            retry_after = self._parse_retry_after(text)
            return True, retry_after
        return False, None
    
    def _run_deep_research_with_retry(self, query: str, schema_wrapper: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œdeep researchï¼Œå‘½ä¸­429æ—¶è‡ªåŠ¨é‡è¯•"""
        delay = self.RATE_LIMIT_INITIAL_DELAY
        for attempt in range(1, self.RATE_LIMIT_MAX_RETRIES + 1):
            try:
                return self._execute_research_once(query, schema_wrapper)
            except Exception as err:
                should_retry, retry_after = self._should_retry_rate_limit(err)
                is_last_attempt = attempt >= self.RATE_LIMIT_MAX_RETRIES
                if not should_retry or is_last_attempt:
                    raise
                
                wait_seconds = retry_after or delay
                logger.warning(
                    f"Deep Research hit OpenAI rate limit (attempt {attempt}/{self.RATE_LIMIT_MAX_RETRIES}); "
                    f"sleeping {wait_seconds:.2f}s before retry"
                )
                time.sleep(wait_seconds)
                delay *= 2
    
    def _contains_non_english(self, text: Optional[str]) -> bool:
        """æ£€æµ‹æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«å¸¸è§çš„éè‹±æ–‡å­—ç¬¦ï¼ˆä¸­ã€æ—¥ã€éŸ©ï¼‰"""
        if not text:
            return False
        return bool(self.NON_ENGLISH_PATTERN.search(text))
    
    def _collect_strings(self, value: Any):
        """é€’å½’éå†ç»“æ„ï¼Œæå–æ‰€æœ‰å­—ç¬¦ä¸²å€¼"""
        if isinstance(value, str):
            yield value
        elif isinstance(value, dict):
            for v in value.values():
                yield from self._collect_strings(v)
        elif isinstance(value, list):
            for item in value:
                yield from self._collect_strings(item)
    
    def _is_english_output(self, raw_answer: str, structured_answer: Optional[Dict[str, Any]]) -> bool:
        """åˆ¤æ–­åŸæ–‡åŠç»“æ„åŒ–å†…å®¹æ˜¯å¦å…¨éƒ¨ä¸ºè‹±æ–‡"""
        if self._contains_non_english(raw_answer):
            return False
        
        if structured_answer:
            for text in self._collect_strings(structured_answer):
                if self._contains_non_english(text):
                    return False
        
        return True
    
    def _format_field_definitions(self, flattened: Dict[str, Dict[str, Any]]) -> str:
        """å°†å¹³é“ºå­—æ®µå®šä¹‰è½¬ä¸ºJSONæ–‡æœ¬ä¾›LLMå‚è€ƒ"""
        serialized = []
        for key, field in flattened.items():
            serialized.append({
                "key": key,
                "canonical_name": field.get("canonical_name", key),
                "description": field.get("description", ""),
                "type": field.get("type", "text"),
                "unit": field.get("unit"),
                "required": field.get("required", True)
            })
        return json.dumps(serialized, ensure_ascii=False, indent=2)
    
    def _load_registry_definitions(self, use_fields: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """ä»registryåŠ è½½use_fieldså®šä¹‰"""
        if not isinstance(use_fields, list) or not use_fields:
            raise ValueError("expected_fields.use_fields ä¸èƒ½ä¸ºç©º")
        
        repository = get_repository(self.settings)
        if not repository:
            raise ValueError("Repository æœªåˆå§‹åŒ–ï¼Œæ— æ³•åŠ è½½ registry å®šä¹‰")
        
        keys = []
        required_map = {}
        for idx, field in enumerate(use_fields):
            if not isinstance(field, dict):
                raise ValueError(f"use_fields[{idx}] å¿…é¡»æ˜¯å¯¹è±¡")
            key = field.get('key')
            if not key:
                raise ValueError(f"use_fields[{idx}] ç¼ºå°‘ key")
            keys.append(key)
            required_map[key] = bool(field.get('required', False))
        
        result = repository.client.table('metric_key_registry')\
            .select('key, canonical_name, description, value_type, unit')\
            .in_('key', keys)\
            .execute()
        
        registry_map = {row['key']: row for row in (result.data or []) if row.get('key')}
        missing = [key for key in keys if key not in registry_map]
        if missing:
            raise ValueError(f"use_fields åŒ…å«æœªæ³¨å†Œå­—æ®µ: {missing}")
        
        flattened = {}
        for key in keys:
            row = registry_map[key]
            flattened[key] = {
                "canonical_name": row.get("canonical_name", key),
                "description": row.get("description", ""),
                "type": row.get("value_type", "text"),
                "unit": row.get("unit"),
                "required": required_map.get(key, False)
            }
        return flattened
    
    def _resolve_expected_fields(self, question: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
        """ç»Ÿä¸€åŠ è½½å­—æ®µå®šä¹‰ï¼ˆæ ‘çŠ¶æˆ–use_fieldsï¼‰"""
        expected_fields = question.get('expected_fields')
        if not expected_fields:
            raise ValueError("expected_fields ç¼ºå¤±ï¼Œæ— æ³•ç”Ÿæˆ structured è¾“å‡º")
        
        if isinstance(expected_fields, dict) and "use_fields" in expected_fields:
            return self._load_registry_definitions(expected_fields["use_fields"])
        
        try:
            flattened = flatten_expected_fields(expected_fields)
        except Exception as exc:
            raise ValueError(f"expected_fields æ— æ³•å±•å¼€: {exc}") from exc
        if not flattened:
            raise ValueError("expected_fields å±•å¼€å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆ structured è¾“å‡º")
        return flattened
    
    def _generate_structured_output(self, report_text: str, question: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨LLMå°†æœ€ç»ˆæŠ¥å‘Šè½¬æ¢ä¸º structured + provenance"""
        flattened = self._resolve_expected_fields(question)
        
        field_def_json = self._format_field_definitions(flattened)
        
        prompt = f"""
You are an information extraction expert. Read the research report and extract the required fields.

## Field Definitions
{field_def_json}

## Output Requirements
- Return ONLY valid JSON without code fences.
- JSON schema:
{{
  "structured": {{
    "<field_key>": <value>
  }},
  "provenance": [
    {{
      "fields": ["<field_key>", "..."],
      "evidence_text": "Verbatim English sentences from the report that justify the value."
    }}
  ]
}}
- For type="range": output {{"min": <number>, "max": <number>}} (numbers only).
- For type="number": output a number (no units or strings).
- For type="boolean": output true or false.
- For type="enum" or "text": output strings exactly as phrased in the report.
- Every field listed above must appear in `structured`. Do not invent fields.
- Each field must have at least one provenance entry referencing verbatim sentences from the report (no paraphrasing, no translation).
- evidence_text must come directly from the report; quote the minimal sentences needed.
- All required fields must be extracted. If truly unavailable, do not invent values; the run should be considered failed.
- Do not add commentary outside the JSON object.

## Research Report
{report_text}
"""
        response = self.post_structure_llm.invoke(prompt)
        raw_content = getattr(response, "content", response)
        if isinstance(raw_content, list):
            raw_content = "\n".join(
                part.get("text", "") for part in raw_content if isinstance(part, dict)
            )
        try:
            parsed = json.loads(raw_content)
        except Exception as e:
            logger.error(f"è§£æ structured è¾“å‡ºå¤±è´¥: {e}")
            raise ValueError("LLM æœªè¿”å›æœ‰æ•ˆJSONç»“æ„")
        
        structured = parsed.get("structured") or {}
        provenance = parsed.get("provenance") or []
        
        required_missing = [
            key for key, field in flattened.items()
            if field.get("required", True) and key not in structured
        ]
        if required_missing:
            raise ValueError(f"ç¼ºå°‘å¿…å¡«å­—æ®µç»“æ„åŒ–ç»“æœ: {required_missing}")
        
        if not provenance:
            raise ValueError("ç”Ÿæˆçš„ provenance ä¸ºç©º")
        
        # è§„èŒƒåŒ– provenance ä¸­çš„å­—æ®µ
        normalized_prov = []
        for entry in provenance:
            fields = entry.get("fields") or []
            evidence_text = entry.get("evidence_text", "").strip()
            valid_fields = [f for f in fields if f in flattened]
            if not valid_fields or not evidence_text:
                continue
            normalized_prov.append({
                "fields": valid_fields,
                "evidence_text": evidence_text
            })
        
        if not normalized_prov:
            raise ValueError("provenance ä¸­æ²¡æœ‰æœ‰æ•ˆæ¡ç›®")
        
        return {
            "structured": structured,
            "provenance": normalized_prov
        }
    
    def _get_schema_from_expected_fields(self, expected_fields_dsl: Dict[str, Any]) -> Dict[str, Any]:
        """ä» DB DSL ç”Ÿæˆ Perplexity å…¼å®¹çš„ JSON Schema"""
        fields = expected_fields_dsl.get('fields', [])
        
        # æ„å»º structured éƒ¨åˆ†çš„ properties
        structured_props = {}
        structured_required = []
        
        for field in fields:
            key = field.get('key')
            f_type = field.get('type')
            desc = field.get('description', '')
            
            json_type = "string"
            if f_type in ["numeric", "number", "float", "int"]:
                json_type = "number"
            elif f_type == "boolean":
                json_type = "boolean"
            elif f_type in ["json", "array", "object"]:
                # å¯¹å¤æ‚ç±»å‹ç®€å•å¤„ç†ä¸º object æˆ– arrayï¼Œæˆ–ä¿æŒå®½æ³›
                json_type = "object" 
            
            structured_props[key] = {
                "type": json_type,
                "description": desc
            }
            structured_required.append(key)
            
        # æ„å»ºå®Œæ•´ Schema (åŒ…å« structured å’Œ provenance)
        schema = {
            "name": "research_result",
            "schema": {
                "type": "object",
                "properties": {
                    "structured": {
                        "type": "object",
                        "properties": structured_props,
                        "required": structured_required,
                        "additionalProperties": False
                    },
                    "provenance": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "fields": {
                                    "type": "array",
                                    "items": {"type": "string"}
                                },
                            "evidence_text": {
                                    "type": "string",
                                    "description": "50-100 words, semantic dense, self-contained explanation for vector retrieval."
                                },
                                "evidence_sources": {
                                    "type": "array",
                                    "items": {"type": "string"}
                                }
                            },
                            "required": ["fields", "evidence_text", "evidence_sources"],
                            "additionalProperties": False
                        }
                    }
                },
                "required": ["structured", "provenance"],
                "additionalProperties": False
            }
        }
        return schema

    def _get_schema_for_question(self, question: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¹æ®é—®é¢˜ç±»å‹ç”Ÿæˆ JSON Schema"""
        
        # 1. ä¼˜å…ˆä½¿ç”¨ expected_fields DSL
        expected_fields = question.get('expected_fields')
        if expected_fields and isinstance(expected_fields, dict) and 'fields' in expected_fields:
            return self._get_schema_from_expected_fields(expected_fields)
            
        # 2. å›é€€åˆ°æ—§é€»è¾‘ (Fallback)
        q_type = question.get('type', 'text')
        
        base_schema = {
            "name": "research_result",
            "schema": {
                "type": "object",
                "properties": {
                    "explanation": { "type": "string", "description": "å¯¹ç­”æ¡ˆçš„è¯¦ç»†è§£é‡Šå’Œä¸Šä¸‹æ–‡" },
                    "confidence": { "type": "string", "enum": ["high", "medium", "low"] }
                },
                "required": ["explanation", "confidence"]
            }
        }
        
        properties = base_schema["schema"]["properties"]
        required = base_schema["schema"]["required"]
        
        if q_type == 'number':
            properties["value"] = { "type": "number", "description": "æå–çš„æ•°å€¼" }
            properties["unit"] = { "type": "string", "description": "æ•°å€¼çš„å•ä½" }
            required.extend(["value", "unit"])
        elif q_type == 'boolean':
            properties["value"] = { "type": "boolean", "description": "æ˜¯/å¦ç»“è®º" }
            required.append("value")
        elif q_type == 'enum':
            properties["value"] = { "type": "string", "description": "é€‰å®šçš„æšä¸¾å€¼" }
            required.append("value")
        else: # text or table
            properties["answer"] = { "type": "string", "description": "è¯¦ç»†çš„æ–‡æœ¬ç­”æ¡ˆ" }
            required.append("answer")
            
        return base_schema

    def build_research_query(self, ym: Dict[str, Any], ym_summary: Dict[str, Any], question: Dict[str, Any]) -> str:
        """æ„å»ºç ”ç©¶æŸ¥è¯¢ - ä¼˜å…ˆä½¿ç”¨ ymq.prompt_templateï¼Œå¸¦å…¼å®¹æ€§æ£€æŸ¥
        
        ä¼˜å…ˆçº§:
        1. ä½¿ç”¨ question['prompt_template'] (å¦‚æœå­˜åœ¨ä¸”éç©º)
           - æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦å ä½ç¬¦
           - å¦‚æœç¼ºå°‘å ä½ç¬¦ï¼Œè‡ªåŠ¨è¡¥å……äº§å“ä¸Šä¸‹æ–‡
        2. å›é€€åˆ°é»˜è®¤æŸ¥è¯¢æ ¼å¼
        """
        try:
            # æå–åŸºæœ¬ä¿¡æ¯ï¼ˆç¡®ä¿ä¸æ˜¯ Noneï¼‰
            ym_name = ym.get('name') or 'N/A'
            ym_desc = ym.get('description') or ym_summary.get("summary") or ""
            ym_category = ym.get('category') or ''
            question_text = question.get('question_text', '')
            
            # â­ ä¼˜å…ˆä½¿ç”¨ ymq.prompt_template
            prompt_template = question.get('prompt_template', '').strip()
            
            if prompt_template:
                # ä½¿ç”¨æ•°æ®åº“ä¸­çš„ prompt_template
                logger.info(f"âœ“ ä½¿ç”¨ ymq.prompt_template (é•¿åº¦: {len(prompt_template)} å­—ç¬¦)")
                
                # â­ å…¼å®¹æ€§æ£€æŸ¥ï¼šæ˜¯å¦åŒ…å«å¿…è¦çš„å ä½ç¬¦
                has_ym_name = '{{YM_NAME}}' in prompt_template
                has_ym_desc = '{{YM_DESC}}' in prompt_template
                has_expected_fields = '{{expected_fields}}' in prompt_template
                
                missing_placeholders = []
                if not has_ym_name:
                    missing_placeholders.append('{{YM_NAME}}')
                if not has_ym_desc:
                    missing_placeholders.append('{{YM_DESC}}')
                if not has_expected_fields:
                    missing_placeholders.append('{{expected_fields}}')
                
                # å¦‚æœç¼ºå°‘å ä½ç¬¦ï¼Œè‡ªåŠ¨è¡¥å……äº§å“ä¸Šä¸‹æ–‡
                if missing_placeholders:
                    logger.warning(f"âš ï¸ prompt_template ç¼ºå°‘å ä½ç¬¦: {missing_placeholders}ï¼Œè‡ªåŠ¨è¡¥å……äº§å“ä¸Šä¸‹æ–‡")
                    
                    # æ„å»ºè¡¥å……çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
                    context_prefix = "# äº§å“ä¿¡æ¯\n\n"
                    
                    if not has_ym_name:
                        if ym_category:
                            context_prefix += f"**äº§å“åç§°**: {ym_name} ({ym_category})\n\n"
                        else:
                            context_prefix += f"**äº§å“åç§°**: {ym_name}\n\n"
                    
                    if not has_ym_desc and ym_desc:
                        context_prefix += f"**äº§å“æè¿°**: {ym_desc}\n\n"
                    
                    context_prefix += "---\n\n# ç ”ç©¶ä»»åŠ¡\n\n"
                    
                    # å°†ä¸Šä¸‹æ–‡å‰ç½®åˆ° prompt_template
                    prompt_template = context_prefix + prompt_template
                    logger.info(f"âœ“ å·²è‡ªåŠ¨è¡¥å……äº§å“ä¸Šä¸‹æ–‡ï¼Œæ–°é•¿åº¦: {len(prompt_template)} å­—ç¬¦")
                
                # æ›¿æ¢å ä½ç¬¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰- ç¡®ä¿æ›¿æ¢å€¼ä¸æ˜¯ None
                query = prompt_template
                query = query.replace('{{YM_NAME}}', ym_name)
                query = query.replace('{{YM_DESC}}', ym_desc)
                
                # æ›¿æ¢ expected_fields å ä½ç¬¦
                expected_fields = question.get('expected_fields', {})
                if expected_fields:
                    expected_fields_json = json.dumps(expected_fields, ensure_ascii=False, indent=2)
                    query = query.replace('{{expected_fields}}', expected_fields_json)
                
                logger.debug(f"ä½¿ç”¨è‡ªå®šä¹‰ prompt_templateï¼Œæœ€ç»ˆæŸ¥è¯¢é•¿åº¦: {len(query)} å­—ç¬¦")
                return self._ensure_language_requirement(query)
            
            # â­ å›é€€ï¼šä½¿ç”¨é»˜è®¤æŸ¥è¯¢æ ¼å¼
            logger.info("âš ï¸ ymq.prompt_template ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤æŸ¥è¯¢æ ¼å¼")
            
            # æ„å»ºç®€æ´çš„æŸ¥è¯¢
            # æ ¼å¼ï¼šå…³äº [äº§å“åç§°] ([ç±»åˆ«])ï¼Œè¯·ç ”ç©¶ï¼š[é—®é¢˜]
            if ym_category:
                query = f"å…³äº {ym_name} ({ym_category})ï¼Œè¯·ç ”ç©¶ï¼š{question_text}"
            else:
                query = f"å…³äº {ym_name}ï¼Œè¯·ç ”ç©¶ï¼š{question_text}"
            
            # å¦‚æœæœ‰æ‘˜è¦ï¼Œå¯ä»¥æ·»åŠ ç®€çŸ­çš„èƒŒæ™¯
            if ym_desc and len(ym_desc) < 200:
                query += f"\n\nèƒŒæ™¯ä¿¡æ¯ï¼š{ym_desc}"
            
            logger.debug(f"æ„å»ºçš„æŸ¥è¯¢ï¼ˆé»˜è®¤æ ¼å¼ï¼‰: {query[:100]}...")
            return self._ensure_language_requirement(query)
            
        except Exception as e:
            logger.error(f"æ„å»ºç ”ç©¶æŸ¥è¯¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # æœ€ç»ˆå›é€€åˆ°æœ€ç®€å•çš„æŸ¥è¯¢
            return self._ensure_language_requirement(question.get('question_text', ''))
    
    def deep_research(
        self,
        ym: Dict[str, Any],
        ym_summary: Dict[str, Any],
        question: Dict[str, Any],
        forced_run_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„æ·±åº¦ç ”ç©¶æµç¨‹ï¼ˆæ–°ç‰ˆï¼šåˆ›å»ºresearch_runï¼‰"""
        run_id = forced_run_id
        
        try:
            # 0. åˆ›å»º ResearchRun è®°å½• (æ–°å¢)
            
            repository = get_repository(self.settings)
            
            ym_db_id = ym.get('id')
            ymq_db_id = question.get('db_id') or question.get('id')
            
            if repository and ymq_db_id is None:
                question_key = question.get('question_id') or question.get('key')
                if question_key:
                    try:
                        db_lookup = repository.client.table('ymq')\
                            .select('id')\
                            .eq('key', question_key)\
                            .limit(1)\
                            .execute()
                        if db_lookup.data:
                            ymq_db_id = db_lookup.data[0]['id']
                            question['db_id'] = ymq_db_id
                            logger.info(f"ğŸ” ä»æ•°æ®åº“åŠ è½½ question_id={question_key} çš„ db_id={ymq_db_id}")
                    except Exception as e:
                        logger.warning(f"æ ¹æ® question_id æŸ¥è¯¢ ymq.id å¤±è´¥: {e}")
            
            if run_id and repository:
                try:
                    repository.client.table('research_run')\
                        .update({
                            'status': 'running',
                            'error_message': None,
                            'is_latest': False,
                            'raw_output': {},
                            'input_payload': {},
                            'model_name': self.deep_research_client.model,
                            'updated_at': datetime.now().isoformat()
                        })\
                        .eq('id', run_id)\
                        .execute()
                    logger.info(f"ğŸ” å¤ç”¨æŒ‡å®š ResearchRun: run_id={run_id}, ymq_db_id={ymq_db_id}")
                except Exception as e:
                    logger.error(f"æ›´æ–°æŒ‡å®š ResearchRun å¤±è´¥: {e}")
                    run_id = None
            
            if run_id is None and repository and ym_db_id and ymq_db_id:
                # â­ ä½¿ç”¨ db_id (æ•°æ®åº“ID) è€Œä¸æ˜¯é€»è¾‘ id
                existing_run = None
                try:
                    existing_run = repository.client.table('research_run')\
                        .select('id')\
                        .eq('ym_id', ym_db_id)\
                        .eq('ymq_id', ymq_db_id)\
                        .order('created_at', desc=True)\
                        .limit(1)\
                        .execute()
                except Exception as e:
                    logger.warning(f"æŸ¥è¯¢ç°æœ‰ ResearchRun å¤±è´¥: {e}")
                
                existing_data = None
                if existing_run and existing_run.data:
                    existing_data = existing_run.data[0]
                
                if existing_data:
                    run_id = existing_data.get('id')
                    try:
                        repository.client.table('research_run')\
                            .update({
                                'status': 'running',
                                'error_message': None,
                                'is_latest': False,
                                'raw_output': {},
                                'input_payload': {},
                                'model_name': self.deep_research_client.model,
                                'updated_at': datetime.now().isoformat()
                            })\
                            .eq('id', run_id)\
                            .execute()
                        logger.info(f"ğŸ” å¤ç”¨ ResearchRun: run_id={run_id}, ymq_db_id={ymq_db_id}")
                    except Exception as e:
                        logger.error(f"å¤ç”¨ç°æœ‰ ResearchRun å¤±è´¥: {e}")
                        run_id = None
                else:
                    run = ResearchRun(
                        ym_id=ym.get('id'),
                        ymq_id=ymq_db_id,  # â­ ä½¿ç”¨æ•°æ®åº“ ID
                        model_name=self.deep_research_client.model,
                        input_payload={},  # ç¨åå¡«å……
                        raw_output={},     # ç¨åå¡«å……
                        status='running',
                        is_latest=False
                    )
                    
                    try:
                        saved_run = repository.save_research_run(run)
                        run_id = saved_run.get('id')
                        logger.info(f"âœ… åˆ›å»º ResearchRun: run_id={run_id}, ymq_db_id={ymq_db_id}")
                    except Exception as e:
                        logger.error(f"åˆ›å»º ResearchRun å¤±è´¥: {e}")
                        # ç»§ç»­æ‰§è¡Œï¼Œä½†æ²¡æœ‰run_id
            else:
                if not question.get('db_id'):
                    logger.warning(f"Question ç¼ºå°‘ db_idï¼Œæ— æ³•åˆ›å»º ResearchRun: question_id={question.get('question_id')}")
            
            # 1. æ„å»ºç ”ç©¶æŸ¥è¯¢
            query = self.build_research_query(ym, ym_summary, question)
            
            # æ›´æ–° input_payload
            if run_id and repository:
                self._update_input_payload(run_id, repository, query)
            
            # 2. ç”Ÿæˆ Schema
            schema_wrapper = self._get_schema_for_question(question)
            
            # 3. æ‰§è¡Œç ”ç©¶
            logger.info(f"å¼€å§‹ç ”ç©¶: YM={ym.get('ym_id')}, Question={question.get('question_id')}")
            logger.info(f"Research Query:\n{query}")
            
            result = self._run_deep_research_with_retry(query, schema_wrapper)
            
            # Extract - Deep_ResearchAgent è¿”å›çš„å­—æ®µåç§°ä¸åŒ
            raw_answer = result.get('raw_answer_text', '')  # Deep_Research ä½¿ç”¨ raw_answer_text
            structured_answer = result.get('structured_answer', {})
            citations = result.get('citations', [])
            
            if not self._is_english_output(raw_answer, structured_answer):
                logger.warning("æ£€æµ‹åˆ°éè‹±æ–‡ç ”ç©¶ç»“æœï¼Œé™„åŠ å¼ºåˆ¶æŒ‡ä»¤åé‡è¯•ä¸€æ¬¡")
                retry_query = self._append_retry_instruction(query)
                if run_id and repository:
                    self._update_input_payload(run_id, repository, retry_query)
                result = self._run_deep_research_with_retry(retry_query, schema_wrapper)
                raw_answer = result.get('raw_answer_text', '')
                structured_answer = result.get('structured_answer', {})
                citations = result.get('citations', [])
                query = retry_query
                
                if not self._is_english_output(raw_answer, structured_answer):
                    logger.error("æ·±åº¦ç ”ç©¶é‡è¯•åä»åŒ…å«éè‹±æ–‡å†…å®¹")
                    raise ValueError("Deep Research output must be in English but is not.")
            
            structured_block = self._generate_structured_output(raw_answer, question)
            
            # Log full raw output for debugging
            logger.info(f"Research Result Raw:\n{raw_answer}")
            logger.info(f"Research Result Structured:\n{json.dumps(structured_block, ensure_ascii=False, indent=2)}")
            
            # Log Usage / Cost
            usage = result.get('usage', {})
            logger.info(f"Deep_Research Usage (Cost): {json.dumps(usage)}")
            
            
            # 4. ä¿å­˜ raw_output (æ–°å¢)
            if run_id and repository:
                try:
                    repository.client.table('research_run')\
                        .update({
                            'raw_output': {
                                'full_response': raw_answer,
                                'structured_answer': structured_block,  # âœ… ä¿®å¤ï¼šæ·»åŠ structured_answer
                                'citations': citations
                            },
                            'input_payload': {'query': query}
                        })\
                        .eq('id', run_id)\
                        .execute()
                    logger.debug(f"âœ… ä¿å­˜ raw_output åˆ° run_id={run_id}")
                except Exception as e:
                    logger.error(f"ä¿å­˜ raw_output å¤±è´¥: {e}")
            
            # 5. è¿”å›ç»“æœ (åŒ…å«run_id)
            final_result = {
                'raw_answer_text': raw_answer,
                'structured_answer': structured_block,
                'citations': citations,
                'run_id': run_id,  # â­ å…³é”®ï¼šä¼ é€’run_id
                'research_metadata': {
                    'model_used': self.deep_research_client.model,
                    'timestamp': datetime.now().isoformat(),
                    'usage': result.get('usage')
                }
            }
            
            return final_result
            
        except Exception as e:
            logger.error(f"æ·±åº¦ç ”ç©¶å¤±è´¥: {e}")
            raise
    
    def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œç ”ç©¶"""
        logger.info("Running research step with incremental saving")
        
        if not context.get("preprocessed", False):
            raise ValueError("æ•°æ®æœªå®Œæˆé¢„å¤„ç†ï¼Œæ— æ³•è¿›è¡Œç ”ç©¶")
        
        ym_list = context.get("yml_list", [])
        question_list = context.get("question_list", [])
        ym_summaries = context.get("ym_summaries", {})
        
        research_results = []
        force_run_map = context.get("force_run_id_map") or {}
        
        # å¾ªç¯å¤„ç†æ‰€æœ‰YMå’Œé—®é¢˜çš„ç»„åˆ
        for ym in ym_list:
            ym_id = ym.get("ym_id")
            ym_summary = ym_summaries.get(ym_id)
            
            if not ym_summary:
                # ä¸ºäº†é²æ£’æ€§ï¼Œå¦‚æœæ²¡æœ‰æ‘˜è¦ï¼Œå°è¯•åªç”¨åç§°
                ym_summary = {"summary": "No summary available"}
                # logger.warning(f"YM {ym_id} æ²¡æœ‰æ‘˜è¦")
            
            for question in question_list:
                question_id = question.get("question_id")
                
                identifier = question.get("question_id") or question.get("key") or str(question.get("id"))
                forced_run_id = force_run_map.get(identifier)
                
                try:
                    logger.info(f"å¤„ç†ç»„åˆ: YM={ym_id}, Question={question_id}")
                    answer = self.deep_research(ym, ym_summary, question, forced_run_id=forced_run_id)
                    
                    result = {
                        'ym_id': ym_id,
                        'ym_db_id': ym.get('id'), # Pass DB ID for Foreign Key
                        'question_id': question_id,
                        'ymq_db_id': question.get('id'), # Pass DB ID for Foreign Key
                        'answer': answer,
                        'run_id': answer.get('run_id')  # â­ æå‡ run_id åˆ°é¡¶å±‚
                    }
                    
                    research_results.append(result)
                    logger.info(f"ç ”ç©¶å®Œæˆ: YM={ym_id}, Question={question_id}")
                    
                except Exception as e:
                    logger.error(f"ç ”ç©¶å¤±è´¥: YM={ym_id}, Question={question_id}: {e}")
                    continue
        
        context["research_results"] = research_results
        logger.info(f"ç ”ç©¶æ­¥éª¤å®Œæˆ: {len(research_results)}ä¸ªç»“æœ")
        return context
